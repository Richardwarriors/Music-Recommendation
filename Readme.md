* üéµ Music-Recommendation

**In-Context Learning (ICL) Music Recommendation using Gemma 3 (4B) with BPR-MF Baseline**

This repository explores how Large Language Models (LLMs)‚Äîspecifically **Gemma 3 (4B)**‚Äîperform music recommendation via **In-Context Learning (ICL)** and **Chain-of-Thought (CoT)** reasoning.
To benchmark LLM performance, we implement a traditional collaborative filtering model using **BPR-MF** and compare it against LLM-based recommendations.

---

## üìÅ Repository Structure

```
Music-Recommendation/
‚îÇ
‚îú‚îÄ‚îÄ BPR/                      
‚îÇ   ‚îú‚îÄ‚îÄ BPR_MF.py                 # BPR-MF model (matrix factorization + BPR loss)
‚îÇ   ‚îú‚îÄ‚îÄ data.py                   # Dataset loader and negative sampling
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Training + evaluation entry point
‚îÇ   ‚îî‚îÄ‚îÄ preprocess_data.py        # Preprocessing for Amazon Digital Music
‚îÇ
‚îú‚îÄ‚îÄ Data/
‚îÇ   ‚îî‚îÄ‚îÄ prompt_ADM.txt            # Dataset-specific prompt template for LLM
‚îÇ
‚îú‚îÄ‚îÄ LLM4Music/
‚îÇ   ‚îú‚îÄ‚îÄ LLM4Rec.py                # Gemma 3 ICL recommendation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ CoT_reasoning_process.txt # Saved reasoning steps from Gemma
‚îÇ   ‚îú‚îÄ‚îÄ CoT_recommendations.txt   # Generated ICL recommendations
‚îÇ   ‚îú‚îÄ‚îÄ Output-NoShot.txt         # Zero-shot results
‚îÇ   ‚îú‚îÄ‚îÄ Output - FewShot.txt      # Few-shot experiment #1
‚îÇ   ‚îú‚îÄ‚îÄ Output - FewShot2.txt     # Few-shot experiment #2
‚îÇ   ‚îî‚îÄ‚îÄ Output_result.txt         # Final processed results summary
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ Project Goal

This project answers:

> **Can an LLM infer user music preferences from a few examples and generate high-quality recommendations, comparable to collaborative filtering?**

We compare:

* **BPR-MF**: Pure collaborative filtering baseline
* **Gemma 3 (4B) ICL**: LLM reasoning over music taste
* **Gemma 3 (4B) CoT**: Adding chain-of-thought explanation + refinement

---

## üß† Methods

### **1Ô∏è‚É£ BPR-MF Baseline**

We implement Bayesian Personalized Ranking with Matrix Factorization.

Included:

* Implicit feedback
* Pairwise ranking loss
* Uniform negative sampling
* HR@K and NDCG@K metrics
* Amazon Digital Music preprocessing

Run training:

```bash
cd BPR
python main.py
```

---

### **2Ô∏è‚É£ LLM Recommendation (Gemma 3:4B)**

Gemma is used to perform:

* Zero-shot
* Few-shot ICL
* Multi-shot preference summarization
* Chain-of-Thought reasoning
* Conversational recommendation refinement

All reasoning and generated outputs are saved under:

```
LLM4Music/CoT_reasoning_process.txt
LLM4Music/CoT_recommendations.txt
LLM4Music/Output-*.txt
```

Run the LLM pipeline:

```bash
cd LLM4Music
python LLM4Rec.py 
```

Example prompt (`prompt_ADM.txt`):

```
Below is the user's music history. Infer their taste and recommend 5 new songs.
Explain your reasoning step-by-step.

User History:
...
```

---

## üìä Evaluation

We evaluate:

### **Quantitative (Baseline Only)**

* HR@10
* NDCG@10

### **Qualitative (LLM)**

* Genre consistency
* Artist similarity
* Mood coherence
* Reasoning correctness
* Multi-turn adaptability

A final comparison table (placeholder):

| Model      | Setting     | HR@10 | NDCG@10 | Notes                    |
| ---------- | ----------- | ----- | ------- | ------------------------ |
| BPR-MF     | CF baseline | TBD   | TBD     | Standard MF + BPR        |
| Gemma 3 4B | Zero-shot   | ‚Äî     | ‚Äî       | Uses no examples         |
| Gemma 3 4B | Few-shot    | ‚Äî     | ‚Äî       | 1‚Äì3 example preferences  |
| Gemma 3 4B | CoT         | ‚Äî     | ‚Äî       | Chain-of-thought enabled |

---

## üöÄ Getting Started

### **Run BPR baseline**

```bash
python BPR/main.py
```

### **Run LLM inference**

```bash
python LLM4Music/LLM4Rec.py
```
